<!DOCTYPE html>


<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta name="description" content="Frederik Ebert personal website">
	<meta name="keywords" content="robotics,research,deep learning,reinforcement learning, control, dexterous manipulation">
	<meta name="author" content="Frederik Ebert">
	<link rel="stylesheet" type="text/css" href="./frederikebert_files/basic-profile.css" title="Basic Profile" media="all">
	<title>Frederik Ebert</title>


    <style>
        th, td {
            padding: 5px;
            margin:0
        }
        p {
            margin-bottom: 5px;
        }


        .one
        {
            width: 160px;
            height: 160px;
            position: relative;
        }
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
    </style>

</head>




<body>
<div id="wrap">
	<div id="sidebar">
		<a href="https://febert.github.io/frederikebert_files/me.jpg"><img src="./frederikebert_files/me.jpg"  width="70%" height="70%" style="border: none;" alt="Frederik Ebert"></a>
        
        <p>Contact: <a href="mailto:febert@berkeley.edu">febert@berkeley.edu</a></p>
        
        <p>Here you can find my <a href="https://febert.github.io/frederikebert_files/curriculum_vitae_frederik_ebert.pdf"> resume </a> </p>
        
	
	</div>

	<div id="content">
		<h2>Frederik Ebert</h2>
		
        <p>
            I'm a PhD candidate in Computer Science at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~svlevine/">Prof. Sergey Levine</a> and <a href="http://ai.stanford.edu/~cbfinn/">Prof. Chelsea Finn (Stanford CS department)</a>.
            In my research at the <a href="http://bair.berkeley.edu/"> Berkeley Artificial Intelligence Laboratory (BAIR) </a>

            I focus on the development of algorithms for robotic manipulation using techniques from deep learning, deep reinforcement learning and classical robotics.
        </p>
            I completed a Bachelor's degree in mechatronics and information technology and a master's degree in "Robotics Cognition Intelligence" at <a href="https://www.tum.de/"> TU Munich (TUM)</a>.
        <p>

        </p>
        
        <p>
            Previously I have worked at the mechatronics institute of the <a href="http://www.dlr.de/rm-neu/"> German Aerospace Center (DLR)</a> on the mechanical design and control system of a quadruped robot.
        </p>


        <h2>Publications</h2>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <img src="./frederikebert_files/gcp_animate.gif" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/2006.13205">
                        <strong>Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors</strong></a><br>
                        Karl Pertsch*, Oleh Rybkin*, <strong>Frederik Ebert</strong>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>, Dinesh Jayaraman, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>

                        <em>Preprint. </em>
                        <!--<em>Thirty-fifth Annual Conference on Neural Information Processing Systems (Neurips)</em>, 2021 <br>-->
                        <a href="https://arxiv.org/abs/2006.13205">arXiv</a>
                        /
                        <a href="https://orybkin.github.io/video-gcp/">project website</a>
                    </p><p></p>
                    <p>
                        We propose a hierarchical prediction model that predicts sequences by recursive infilling. We use this model to devise a hierarchical planning approach that allows to scale visual MPC to long-horizon tasks with hundreds of time steps.
                    </p>
                </td>
            </tr>

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="TAP" style="opacity: 0;"><img src="./frederikebert_files/omnitact.gif" width="150"></div>
                        <img src="./frederikebert_files/omnitact.gif" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/2003.06965">
                        <strong>OmniTact: A Multi-Directional High Resolution Touch Sensor</strong></a><br>
                        Akhil Padmanabha, <strong >Frederik Ebert</strong>, Stephen Tian, Roberto Calandra, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a> <br />
                        <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2020 <br>
                        <a href="https://arxiv.org/abs/2003.06965">arXiv</a>
                        /
                        <a href="https://sites.google.com/berkeley.edu/omnitact">project website</a> / <a href="https://bair.berkeley.edu/blog/2020/05/14/omnitact"> blog post</a>
                    </p><p></p>
                    <p>
                        OmniTact is a novel high-resolution multi-directional tactile sensor based on the Gelsight sensor. We show that the omnidirectional sensing capabilities allow inserting an eletrical connector purely based on the sense of touch.
                    </p>
                </td>
            </tr>

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="TAP" style="opacity: 0;"><img src="./frederikebert_files/robonet.png" width="150"></div>
                        <img src="./frederikebert_files/robonet.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/pdf/1910.11215.pdf">
                        <strong>RoboNet: Large-Scale Multi-Robot Learning</strong></a><br>
                        <a>Sudeep Dasari</a>, <strong >Frederik Ebert</strong>,  Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a> <br />
                        <em>Conference on Robot Learning (CoRL)</em>, 2019 <br>
                        <a href="https://arxiv.org/pdf/1910.11215.pdf">arXiv</a>
                        /
                        <a href="https://www.robonet.wiki/">project website</a>
                    </p><p></p>
                    <p>
                        RoboNet is a large-scale database for sharing robotic experience across different robots for learning generalizable skills. RoboNet contains datafrom 7 different robot platforms and allows transferring skill and dynamics models between different robots.
                    </p>
                </td>
            </tr>

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="TAP" style="opacity: 0;"><img src="./frederikebert_files/TAP.png" width="150"></div>
                        <img src="./frederikebert_files/TAP.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1808.07784">
                        <strong>Time-Agnostic Prediction: Predicting Predictable Video Frames</strong></a><br>
                        <a>Dinesh Jayaraman</a>, <strong >Frederik Ebert</strong>, <a>Alexei A. Efros</a> <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br />
                        <em>International Conference on Learning Representations (ICLR)</em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1808.07784">arXiv</a>
                        /
                        <a href="https://sites.google.com/view/ta-pred">video results and data</a>
                    </p><p></p>
                    <p>
                        Time agnostic prediction (TAP) is a method for predicting intermediate images in between a start frame and a goal frame for the purpose of planning.
                        Instead of predicting at fixed time-intervals the optimizer chooses the optimal time-step.
                    </p>
                </td>
            </tr>

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="tooluse" style="opacity: 0;"><img src="./frederikebert_files/tooluse.png" width="150"></div>
                        <img src="./frederikebert_files/tooluse.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1904.05538">
                        <strong>Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</strong></a><br>
                        <a>Annie Xie</a>, <strong >Frederik Ebert</strong>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a> <br />
                        <em>Robotics: Science and Systems (RSS) </em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1904.05538">arXiv</a>
                        /
                        <a href="https://sites.google.com/view/anonymous-dgvf/home">video results and data</a> / <a href="https://bair.berkeley.edu/blog/2019/04/11/tools/"> blog post</a>
                    </p><p></p>
                    <p>
                        We combine diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks.
                    </p>
                </td>
            </tr>

            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="tactile_mpc" style="opacity: 0;"><img src="./frederikebert_files/tactile_mpc.png" width="150" ></div>
                        <img src="./frederikebert_files/tactile_mpc.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1903.04128">
                        <strong>Manipulation by Feel: Touch-Based Control with Deep Predictive Models</strong></a><br>
                        <a>Stephen Tian*</a>, <strong >Frederik Ebert*</strong>,  <a>Dinesh Jayaraman</a>, <a>Dinesh Jayaraman</a>,<a>Mayur Mudigonda</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>,<a>Roberto Calandra</a> ,<a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> (* equal contribution) <br/>
                        <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1903.04128">arXiv</a>
                        <!--/-->
                        <!--<a href="https://github.com/febert/robustness_via_retrying">code</a>-->
                        /
                        <a href="https://sites.google.com/view/manipbyfeel/">video results and data</a> / <a href="https://bair.berkeley.edu/blog/2019/03/21/tactile"> blog post</a>
                    </p><p></p>
                    <p>
                        We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision.
                        We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball and 20-sided die
                    </p>
            </td>
            </tr>


            <tr>
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="retrying_gif" style="opacity: 0;"><img src="./frederikebert_files/retrying.png" width="150" height="150"></div>
                        <img src="./frederikebert_files/retrying.gif" width="150" height="150">
                    </div>
                    <script type="text/javascript">
                        function retrying_start() {
                            document.getElementById('retrying_gif').style.opacity = "1";
                        }
                        function retrying_stop() {
                            document.getElementById('retrying_gif').style.opacity = "0";
                        }
                        retrying_stop()
                    </script>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1810.03043.pdf">
                        <strong>Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning</strong></a><br>
                        <strong >Frederik Ebert</strong>,  <a>Sudeep Dasari</a>, <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>
                        <em>Conference on Robot Learning (CoRL)</em>, 2018 <br>
                        <a href="https://arxiv.org/abs/1810.03043">arXiv</a>
                        /
                        <a href="https://github.com/febert/robustness_via_retrying">code</a>
                        /
                        <a href="https://sites.google.com/view/robustness-via-retrying/home">video results and data</a>
                    </p><p></p>
                    <p>
                        To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation.
                    </p>
                </td>
            </tr>


            <tr onmouseout="sna_stop()" onmouseover="sna_start()">
                <td valign="top" width="25%">
                    <div class="one">
                        <div class="two" id="sna_gif" style="opacity: 0;"><img src="./frederikebert_files/oadna_apart_square.gif" width="150" height="150"></div>
                        <img src="./frederikebert_files/oadna_frame.gif" width="150" height="150">
                    </div>
                    <script type="text/javascript">
                        function sna_start() {
                            document.getElementById('sna_gif').style.opacity = "1";
                        }
                        function sna_stop() {
                            document.getElementById('sna_gif').style.opacity = "0";
                        }
                        sna_stop()
                    </script>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/pdf/1710.05268.pdf">
                        <strong>Self-Supervised Visual Planning with Temporal Skip Connections</strong></a><br>
                        <strong >Frederik Ebert</strong>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>, <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>,
                        <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
                        <em>Conference on Robot Learning (CoRL)</em>, 2017 <font color="green"><strong>(Long Talk)</strong></font> <br>
                        <a href="https://arxiv.org/abs/1710.05268">arXiv</a>
                        /
                        <a href="https://github.com/febert/visual_mpc">code</a>
                        /
                        <a href="https://sites.google.com/view/sna-visual-mpc">video results and data</a>
                    </p><p></p>
                    <p>
                        We present three simple improvements to self-supervised visual foresight algorithm that lead to substantially better visual planning capabilities. Our
                        method can perform tasks that require longer-term planning and involve multiple objects.
                    </p>
                </td>
            </tr>


        </table>


        
		<p class="credits">© 2020 Frederik Ebert<br>
	</div>

</div>




</body></html>
