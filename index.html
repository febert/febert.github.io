<!DOCTYPE html>


<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<meta name="description" content="Frederik Ebert personal website">
	<meta name="keywords" content="robotics,research,deep learning,reinforcement learning, control, dexterous manipulation">
	<meta name="author" content="Frederik Ebert">
	<link rel="stylesheet" type="text/css" href="./frederikebert_files/basic-profile.css" title="Basic Profile" media="all">
	<title>Frederik Ebert</title>


    <style>
        /*th, td {*/
            /*padding: 6px;*/
        /*}*/
        .one
        {
            width: 160px;
            height: 160px;
            position: relative;
        }
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
    </style>

</head>




<body>
<div id="wrap">
	<div id="sidebar">
		<a href="https://febert.github.io/frederikebert_files/me.jpg"><img src="./frederikebert_files/me.jpg"  width="70%" height="70%" style="border: none;" alt="Frederik Ebert"></a>
        
        <p>Contact: <a href="mailto:febert@berkeley.edu">febert@berkeley.edu</a></p>
        
        <p>Here you can find my <a href="https://febert.github.io/frederikebert_files/curriculum_vitae_frederik_ebert.pdf"> resume </a> </p>
        
	
	</div>

	<div id="content">
		<h2>Frederik Ebert</h2>
		
        <p>
            I'm a PhD student in Computer Science at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~svlevine/">Prof. Sergey Levine</a>
            and part of the  <a href="http://bair.berkeley.edu/"> Berkeley Artificial Intelligence Laboratory (BAIR) </a>.

            In my research I focus on the development of algorithms for robotic manipulation using techniques from deep learning, deep reinforcement learning and classical robotics.
        </p>
            I completed a Bachelor's degree in mechatronics and information technology and a master's degree in "Robotics Cognition Intelligence" at <a href="https://www.tum.de/"> TU Munich (TUM)</a>.
        <p>

        </p>
        
        <p>
            Previously I have worked at the mechatronics institute of the <a href="http://www.dlr.de/rm-neu/"> German Aerospace Center (DLR)</a> on the mechanical design and control system of a quadruped robot.
        </p>

        <h3>Robotic Interaction Datasets</h3>

        We are maintaining a growing database of open robotic interaction datasets, which you can find <a href="https://sites.google.com/berkeley.edu/robotic-interaction-datasets" >here</a>.
        <br/>
        <br/>

        <h3>Blog Posts</h3>

        <ul>
            <li> <a href="https://bair.berkeley.edu/blog/2019/04/11/tools/">Robots that Learn to Use Improvised Tools</a>: how robots can figure out how to solve tasks using tools, including unconventional tools, by learning from a combination of unsupervised interaction and example demonstrations.</li>
            <li> <a href="https://bair.berkeley.edu/blog/2019/03/21/tactile/">Manipulation By Feel</a>: how we can use visual foresight to enable robots to manipulate objects entirely by feel without vision.</li>
            <li> <a href="http://bair.berkeley.edu/blog/2018/11/30/visual-rl/">Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots</a>: overviews our work on learning a single model that can be used to accomplish many different tasks, without requiring human supervision. </li>
        </ul>
        <br/>

        <h2>Publications</h2>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr onmouseout="retrying_stop()" onmouseover="retrying_start()">
                <td width="25%">
                    <div class="one">
                        <div class="two" id="TAP" style="opacity: 0;"><img src="./frederikebert_files/TAP.png" width="150"></div>
                        <img src="./frederikebert_files/TAP.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1808.07784">
                        <strong>Time-Agnostic Prediction: Predicting Predictable Video Frames</strong></a><br>
                        <a>Dinesh Jayaraman</a>, <strong >Frederik Ebert</strong>, <a>Alexei A. Efros</a> <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br />
                        <em>International Conference on Learning Representations (ICLR) </em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1808.07784">arXiv</a>
                        /
                        <a href="https://sites.google.com/view/ta-pred">video results and data</a>
                    </p><p></p>
                    <p>
                        Time agnostic prediction (TAP) is a method for predicting intermediate images in between a start frame and a goal frame for the purpose of planning.
                        Instead of predicting at fixed time-intervals the optimizer chooses the optimal time-step.
                    </p>
                </td>
            </tr>

            <tr onmouseout="retrying_stop()" onmouseover="retrying_start()">
                <td width="25%">
                    <div class="one">
                        <div class="two" id="tooluse" style="opacity: 0;"><img src="./frederikebert_files/tooluse.png" width="150"></div>
                        <img src="./frederikebert_files/tooluse.png" width="150">
                    </div>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1904.05538">
                        <strong>Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</strong></a><br>
                        <a>Annie Xie</a>, <strong >Frederik Ebert</strong>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a> <br />
                        <em>Robotics: Science and Systems (RSS) </em>, 2019 <br>
                        <a href="https://arxiv.org/abs/1904.05538">arXiv</a>
                        /
                        <a href="https://sites.google.com/view/anonymous-dgvf/home">video results and data</a>
                    </p><p></p>
                    <p>
                        We combine diverse demonstration data with self-supervised interaction data, aiming to leverage the interaction data to build generalizable models and the demonstration data to guide the model-based RL planner to solve complex tasks.
                    </p>
                </td>
            </tr>


            <td width="25%">
                <div class="one">
                    <div class="two" id="tactile_mpc" style="opacity: 0;"><img src="./frederikebert_files/tactile_mpc.png" width="150" ></div>
                    <img src="./frederikebert_files/tactile_mpc.png" width="150">
                </div>

            </td>
            <td valign="top" width="75%">
                <p><a href="https://arxiv.org/abs/1903.04128">
                    <strong>Manipulation by Feel: Touch-Based Control with Deep Predictive Models</strong></a><br>
                    <a>Stephen Tian*</a>, <strong >Frederik Ebert*</strong>,  <a>Dinesh Jayaraman</a>, <a>Dinesh Jayaraman</a>,<a>Mayur Mudigonda</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>,<a>Roberto Calandra</a> ,<a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> (* equal contribution) <br/>
                    <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2019 <br>
                    <a href="https://arxiv.org/abs/1903.04128">arXiv</a>
                    <!--/-->
                    <!--<a href="https://github.com/febert/robustness_via_retrying">code</a>-->
                    /
                    <a href="https://sites.google.com/view/manipbyfeel/">video results and data</a>
                </p><p></p>
                <p>
                    We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision.
                    We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball and 20-sided die
                </p>
            </td>


            <tr onmouseout="retrying_stop()" onmouseover="retrying_start()">
                <td width="25%">
                    <div class="one">
                        <div class="two" id="retrying_gif" style="opacity: 0;"><img src="./frederikebert_files/retrying.png" width="150" height="150"></div>
                        <img src="./frederikebert_files/retrying.gif" width="150" height="150">
                    </div>
                    <script type="text/javascript">
                        function retrying_start() {
                            document.getElementById('retrying_gif').style.opacity = "1";
                        }
                        function retrying_stop() {
                            document.getElementById('retrying_gif').style.opacity = "0";
                        }
                        retrying_stop()
                    </script>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/abs/1810.03043.pdf">
                        <strong>Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning</strong></a><br>
                        <strong >Frederik Ebert</strong>,  <a>Sudeep Dasari</a>, <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>
                        <em>Conference on Robot Learning (CoRL)</em>, 2018 <br>
                        <a href="https://arxiv.org/abs/1810.03043">arXiv</a>
                        /
                        <a href="https://github.com/febert/robustness_via_retrying">code</a>
                        /
                        <a href="https://sites.google.com/view/robustness-via-retrying/home">video results and data</a>
                    </p><p></p>
                    <p>
                        To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation.
                    </p>
                </td>
            </tr>


            <tr onmouseout="sna_stop()" onmouseover="sna_start()">
                <td width="25%">
                    <div class="one">
                        <div class="two" id="sna_gif" style="opacity: 0;"><img src="./frederikebert_files/oadna_apart_square.gif" width="150" height="150"></div>
                        <img src="./frederikebert_files/oadna_frame.gif" width="150" height="150">
                    </div>
                    <script type="text/javascript">
                        function sna_start() {
                            document.getElementById('sna_gif').style.opacity = "1";
                        }
                        function sna_stop() {
                            document.getElementById('sna_gif').style.opacity = "0";
                        }
                        sna_stop()
                    </script>

                </td>
                <td valign="top" width="75%">
                    <p><a href="https://arxiv.org/pdf/1710.05268.pdf">
                        <strong>Self-Supervised Visual Planning with Temporal Skip Connections</strong></a><br>
                        <strong >Frederik Ebert</strong>, <a href="http://people.eecs.berkeley.edu/~cbfinn/" >Chelsea Finn</a>, <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>,
                        <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
                        <em>Conference on Robot Learning (CoRL)</em>, 2017 <font color="green"><strong>(Long Talk)</strong></font> <br>
                        <a href="https://arxiv.org/abs/1710.05268">arXiv</a>
                        /
                        <a href="https://github.com/febert/visual_mpc">code</a>
                        /
                        <a href="https://sites.google.com/view/sna-visual-mpc">video results and data</a>
                    </p><p></p>
                    <p>
                        We present three simple improvements to self-supervised visual foresight algorithm that lead to substantially better visual planning capabilities. Our
                        method can perform tasks that require longer-term planning and involve multiple objects.
                    </p>
                </td>
            </tr>


        </table>


        
		<p class="credits">© 2018 Frederik Ebert<br>
	</div>

</div>




</body></html>
